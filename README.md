# Optimizing-Gradient-Descent
Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent.

<B> List of Gradient Descent Algorithms : </B>
Gradient descent variants
Batch gradient descent
Stochastic gradient descent
Mini-batch gradient descent
Challenges
Gradient descent optimization algorithms
Momentum
Nesterov accelerated gradient
Adagrad
Adadelta
RMSprop
Adam
AdaMax
Nadam
AMSGrad
Visualization of algorithms
Which optimizer to choose?
Parallelizing and distributing SGD
Hogwild!
Downpour SGD
Delay-tolerant Algorithms for SGD
TensorFlow
Elastic Averaging SGD
Additional strategies for optimizing SGD
Shuffling and Curriculum Learning
Batch normalization
Early Stopping
Gradient noise
